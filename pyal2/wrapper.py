#!/usr/bin/env python

from datetime import datetime
from datetime import timedelta
import argparse
import os
import sys
import time
import numpy as np
import traceback

try:
    import coloredlogs, logging
except ImportError:
    import logging

# In case the package has not been properly installed (with pip install) we make sure that the folder "pyal2" is in the import path
sys.path.append(os.path.dirname(os.path.realpath(__file__)) + '/self_import/.')
import pyal2
from pyal2.exit_status import exit_status
from pyal2.chunking import chunk_2D, chunk_1D
from pyal2.parallel import ExceptionInSubprocessWrapper, chunk_init
from multiprocessing import Pool, Lock
from pyal2.data_store import DataStore
from pyal2.al2_runner import Al2Runner
from pyal2 import __version__
from pyal2.config_utils.sensor_constants import sensor_constants

# TODO : some logic about handling dates is located in data_store.py. It maybe need to be merged with this function.
def parse_outputdates(outputdates, keywords=None):
    """ Parse the outputdates parameter.
    >>> parse_outputdates(['fromsensor:AVHRR_NOAA11'])
    [['20-11-1988', '20-09-1994']]
    >>> parse_outputdates(['fromsensor-spinoff:AVHRR_NOAA11'])
    [['20-11-1988', '20-11-1989']]

    >>> parse_outputdates(['19-03-1977','14-02-1988'])
    [['19-03-1977', '14-02-1988']]

    >>> parse_outputdates(['19-03-1977,14-02-1988'])
    [['19-03-1977', '14-02-1988']]

    >>> parse_outputdates(['15-04-2004,15-05-2004','25-04-1997,15-06-1999'])
    [['15-04-2004', '15-05-2004'], ['25-04-1997', '15-06-1999']]

    >>> parse_outputdates(['19-03-1977'])
    [['19-03-1977', '19-03-1977']]
    """
    if not isinstance(outputdates, list):
        raise Exception(f'Input error : outputdates must be a list (and not of type {type(outputdates)})')

    if keywords is None: keywords = {}

    if len(outputdates) == 1 and not ',' in outputdates[0]:
        if not ':' in outputdates[0]:
            return [[outputdates[0], outputdates[0]]]
        
        typ, sensorname = outputdates[0].split(':')

        if typ.lower() == 'fromsensor':
            return [[sensor_constants[sensorname]['date_start'],sensor_constants[sensorname]['date_end']]]
        elif typ.lower() == 'fromsensor-spinoff':
            return [[sensor_constants[sensorname]['date_start'],sensor_constants[sensorname]['date_end_spinoff']]]
        else:
            raise Exception(f'Input error : Unknown type {typ}')

    if len(outputdates) == 2 and (not ',' in outputdates[0]) and (not ',' in outputdates[1]):
        # format seems to be 15-04-2004 15-05-2004
        return [outputdates]

    # format seems to be 15-04-2004,15-05-2004 25-04-1997,15-06-1999
    return [outputdates[i].split(',') for i in range(len(outputdates))]

def parse_args():
        parser = argparse.ArgumentParser(
                description='Apply BRDF model fitting to reflectance, with various Kalman filters, and generate brdf and albedos.',
                formatter_class=argparse.ArgumentDefaultsHelpFormatter)

        parser.add_argument('acf', help='Algorithm config file')
        parser.add_argument('pcf', help='Product config file')
        parser.add_argument('-i', '--instruments', nargs='+', help='List of instrument we want to run')
        parser.add_argument('-f', '--config-format', help='Format of the configuration file : yaml (default) or f90nml_msg or f90nml_c3s', default='yaml')
        parser.add_argument('--outputdates', nargs='+', help='date start and end for expected albedo outputs')
        parser.add_argument('--startseries', help='force value of start series in algo config (valid values are true/false)')
        parser.add_argument('--checkpoint', nargs='+', help='force to start from a checkpoint (must provide a VEG and ALBEDO file generated by this MDALN)')
        parser.add_argument('-x','--window-lat', type=int, nargs='+', help='xmin xmax [xstep not implemented] range to process, starting at 1.')
        parser.add_argument('-y','--window-lon', type=int, nargs='+', help='ymin ymax [ystep not implemented] range to process, starting at 1.')
        parser.add_argument('-w','--window-predefined', help='predefined window name (experimental)')
        parser.add_argument('-d','--debuglevel', type=int, help='debug level. higher means more debug information (also mean more memory and slower execution)', default=0)
        parser.add_argument('-l', '--loglevel', help='log level. CRITICAL ERROR WARNING INFO or DEBUG', default='ERROR')
        parser.add_argument('--no-files', action='store_true', help='the option --no-files avoid using files. It will NOT save any output and checkpoints in files', default=False)
        parser.add_argument('--outdir', help="output analysed veg and albedo files (and debug files if relevant) (ex: 'data-test/{date_Y_m_d}'")
        parser.add_argument('--keywords', nargs='+', help="a list of pairs 'keyword value' that will be replaced \
                            in the config files before setting up the config. Could be useful to use the same    \
                            config file for different runs (to use only two config files for several dates,      \
                            several tiles, several experiment setup, etc.)")
        parser.add_argument('--no-append', action='store_false', help='Do not append to output files it they exists (not recommended)')
        parser.add_argument('--cpu', type=int, help='Nb of CPU to use', default=1)
        parser.add_argument('--chunksize', type=int, nargs='+', help='Data chunk size list for each instrument. Give no value to process the whole domain', default=None)
        parser.add_argument('--nice', type=int, help='Lower the priority of the process (nice +value)', default=0)
        parser.add_argument('--version', action='store_true', help='Show version information and exits')
        args = parser.parse_args()

        if args.version:
            print('Version : ' + __version__)
            exit()

        # Processing args and config
        #if args.sat and args.sat.upper() == 'C3S': args.config_format = 'f90nml_c3s' # for backward compatibility
        if (not args.config_format == 'yaml') and (args.instruments[0] and args.instruments[0].upper() == 'MSG'): 
            args.config_format = 'f90nml_msg' # for backward compatibility

        if (not args.instruments): 
            logging.warning('---- WARNING no instrument was given in options, PCF file instrument list will be used and \
                                   outputdate list should be the same length  --------')

        if args.keywords is None: args.keywords = {}
        else: args.keywords = dict(zip(args.keywords[0::2], args.keywords[1::2])) # transform to dict

        # outputdates preprocessing to make it more robust to different input
        args.outputdates = parse_outputdates(args.outputdates, keywords=args.keywords)

        os.nice(args.nice)
        coloredlogs.install(level=args.loglevel.upper())
        return args

def main():
    """ This is the main function. It selects the right runner according 
        the the sensor and run it (run by chunks if required). """
    # TODO : The whole process of chunking could be done using a combination 
    # of xarray and dask. This would be more reliable and cleaner.

    # parse command line arguments
    args = parse_args()

    # check informations about instruments we will treat:
    logging.info(f'Instruments to process :  We gave a instrument list to process :{args.instruments}')
    instr_list = args.instruments
    chunk_list = args.chunksize

    # parse config files and mix it with command lines arguments
    dstore = DataStore()
    # TODO : check if we could write this part better
    if args.instruments:
        dstore.load_config(args.acf, args.pcf,
                           startseries=args.startseries,
                           dates=args.outputdates,
                           debuglevel=args.debuglevel,
                           window_predefined=args.window_predefined,
                           window_lat=args.window_lat,
                           window_lon=args.window_lon,
                           keywords=args.keywords,
                           config_format=args.config_format, 
                           instruments=args.instruments)
    else:
        dstore.load_config(args.acf, args.pcf,
                           startseries=args.startseries,
                           dates=args.outputdates,
                           debuglevel=args.debuglevel,
                           window_predefined=args.window_predefined,
                           window_lat=args.window_lat,
                           window_lon=args.window_lon,
                           keywords=args.keywords,
                           config_format=args.config_format)

    logdir = dstore.get('globalconfig', {}).get('logdir','.')
    # save DataStore for logging purposes
    dstore.to_yaml(f'{logdir}/dstore.yaml')
    # reload to double check it is well saved
    other_dstore = DataStore()
    other_dstore.load_full_yaml(f'{logdir}/dstore.yaml')
    other_dstore.to_yaml(f'{logdir}/dstore-copy.yaml')
    dstore = other_dstore

    for k, instrument in enumerate(instr_list):

        # Create chunks of data to process
        if chunk_list is not None:
            chunks = chunk_2D(dstore['xfullslice'], dstore['yfullslice'], chunk_list[k], chunk_list[k])
        else:
            chunks = chunk_2D(dstore['xfullslice'], dstore['yfullslice'], None, None)
        # Run the code, in parallel if required
        if args.cpu > 1: # if parallel processing is required, use a Pool of process to run several runner in parallel, calling "process_one_chunk"

            global write_lock
            write_lock = Lock()
            with Pool(args.cpu, initializer=chunk_init, initargs=(write_lock,)) as p:
                results = p.starmap(process_one_chunk, [(instrument, i, c, args, dstore) for i,c in enumerate(chunks)])
            for result in results:
                if isinstance(result, ExceptionInSubprocessWrapper):
                    logging.error('Exception occured in chunk ' + str(result.info) + '"')
                    result.log_error()
                    exit_status('UNABLE_TO_PROCESS')
        else: # if no parallelization is requested (or if there is no chunking), run sequentially, using "process_one_chunk_no_parallel"

            logging.info('                                          ')
            logging.info('Now treating instrument ' + str(instrument))

            for c in chunks:
                result = process_one_chunk_no_parallel(instrument, c, args, dstore)
                if isinstance(result, ExceptionInSubprocessWrapper):
                    logging.warn('Exception occured in the chunk ' + str(result.info) + '"')
                    result.log_error()
                    exit_status('UNABLE_TO_PROCESS')

    exit_status("PROCESS_OK")


def process_one_chunk(instrument, ichunk, chunk, args, dstore):
    ''' This function process one chunk of data. Must be run in multiprocessing
    mode (it uses the global variable "write_lock" which is defined by the pool when calling this function) '''
    # This part may be useful we observe that all processes are slowed
    # down because they are all starting at the same time and all reading
    #concurently the same file.  It has not been well-tested and seems not to be required.
    # It is currently disabled.
    if False:
        #random delay between 0 and 10 sec to prevent process to read the same file simultaneously
        np.random.seed()
        delay = np.random.random() * 10.
        delay = ichunk % 10 + np.random.random() * 60.
        print(str(chunk) + ' : delay = ' + str(delay))
        time.sleep(delay)

    logging.warn(str(ichunk) + ' : Running ' + str(chunk))
    runner = Al2Runner(dstore=dstore, write_lock=write_lock)
    result = runner.process(instrument, chunk[0], chunk[1])
    logging.warn(str(ichunk) + ' : Finished running ' + str(chunk))
    return result

def process_one_chunk_no_parallel(instrument, chunk, args, dstore):
    ''' This function process one chunk of data, no write lock is applied (no multiprocess).'''
    logging.warn('Running ' + str(chunk))
    runner = Al2Runner(dstore=dstore)
    result = runner.process(instrument, chunk[0], chunk[1])

    logging.warn('Finished running ' + str(chunk))
    return result

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        exit_status("UNABLE_TO_PROCESS")
